Validación cruzada

from sklearn.model_selection import StratifiedKFold
def validacion_cruzada(x_train,y_train,kfolds,model):
    cv = StratifiedKFold(n_splits=kfolds,shuffle=False)
    prom=[]
    for train,test in cv.split(x_train,y_train):
    
        modelo=model.fit(x_train.iloc[train],y_train.iloc[train])
        prom.append(eval_model(x_train.iloc[test],x_train.iloc[train] ,y_train.iloc[test],y_train.iloc[train],modelo))

    return(prom)  

# Se realiza validación cruzada pasando cuantas particiones del conjuto de datos quiere entrenar
kflods = 4
ab_vec=validacion_cruzada(X_train,y_train, kflods, modelo1)

print('-------------------------')
display(ab_vec)
print('Promedio', sum(ab_vec)/len(ab_vec)) 
print('-------------------------')



Parámetros Árbol

%%time
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score,precision_score, balanced_accuracy_score


def _score_func(estimator, X, y):
  y_pred_test = estimator.predict(X)
  return balanced_accuracy_score(y, y_pred_test)


class Class_Fit(object):
    def __init__(self, clf, params=None):
        if params:            
            self.clf = clf(**params)
        else:
            self.clf = clf()

    def train(self, x_train, y_train):
        self.clf.fit(x_train, y_train)

    def predict(self, x):
        return self.clf.predict(x)
    
    def grid_search(self, parameters, Kfold):
        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, cv = Kfold,scoring=_score_func)
        
    def grid_fit(self, X, Y):
        self.grid.fit(X, Y)
        
    def grid_predict(self, X, Y):
        self.predictions = self.grid.predict(X)
        print("Precision: {:.2f} % ".format(100*metrics.balanced_accuracy_score(Y, self.predictions)))

from sklearn.model_selection import GridSearchCV
# Se utiliza gridsearch como modelo de selección para determinar los mejores parametros en XGBoost


modelo1.fit(X_train, y_train)
gb = Class_Fit(clf = tree.DecisionTreeClassifier)
param_grid = {
             'criterion':['entropy','gini'],
             'max_depth':[3,4,5,10,20],
             'min_samples_leaf':[5,11,3]
             }
gb.grid_search(parameters = param_grid, Kfold = 4)
gb.grid_fit(X = X_train, Y = y_train)

print("Parámetros árbol:",gb.grid.best_params_)
mejor_arbol=gb.grid.best_estimator_

Parámetros RandomForest

gb = Class_Fit(clf = RandomForestClassifier)
param_grid = {
             'criterion':['entropy','gini'],
             'max_depth':[3,4,5,10,20],
             'min_samples_leaf':[5,11,3,1],
             'min_impurity_decrease':[0,0.025,0.0025],
             'min_samples_split':[2,4,10],
             'n_estimators':[100],
              'max_features':['auto', 'sqrt', 'log2'],
              'class_weight':['balanced', 'balanced_subsample']
             }

gb.grid_search(parameters = param_grid, Kfold = 4)
gb.grid_fit(X = X_train, Y = y_train)

