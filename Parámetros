Validación cruzada

from sklearn.model_selection import StratifiedKFold
def validacion_cruzada(x_train,y_train,kfolds,model):
    cv = StratifiedKFold(n_splits=kfolds,shuffle=False)
    prom=[]
    for train,test in cv.split(x_train,y_train):
    
        modelo=model.fit(x_train.iloc[train],y_train.iloc[train])
        prom.append(eval_model(x_train.iloc[test],x_train.iloc[train] ,y_train.iloc[test],y_train.iloc[train],modelo))

    return(prom)  

# Se realiza validación cruzada pasando cuantas particiones del conjuto de datos quiere entrenar
kflods = 4
ab_vec=validacion_cruzada(X_train,y_train, kflods, modelo1)

print('-------------------------')
display(ab_vec)
print('Promedio', sum(ab_vec)/len(ab_vec)) 
print('-------------------------')



Parámetros Árbol

%%time
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score,precision_score, balanced_accuracy_score


def _score_func(estimator, X, y):
  y_pred_test = estimator.predict(X)
  return balanced_accuracy_score(y, y_pred_test)


class Class_Fit(object):
    def __init__(self, clf, params=None):
        if params:            
            self.clf = clf(**params)
        else:
            self.clf = clf()

    def train(self, x_train, y_train):
        self.clf.fit(x_train, y_train)

    def predict(self, x):
        return self.clf.predict(x)
    
    def grid_search(self, parameters, Kfold):
        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, cv = Kfold,scoring=_score_func)
        
    def grid_fit(self, X, Y):
        self.grid.fit(X, Y)
        
    def grid_predict(self, X, Y):
        self.predictions = self.grid.predict(X)
        print("Precision: {:.2f} % ".format(100*metrics.balanced_accuracy_score(Y, self.predictions)))

from sklearn.model_selection import GridSearchCV
# Se utiliza gridsearch como modelo de selección para determinar los mejores parametros en XGBoost


modelo1.fit(X_train, y_train)
gb = Class_Fit(clf = tree.DecisionTreeClassifier)
param_grid = {
             'criterion':['entropy','gini'],
             'max_depth':[3,4,5,10,20],
             'min_samples_leaf':[5,11,3]
             }
gb.grid_search(parameters = param_grid, Kfold = 4)
gb.grid_fit(X = X_train, Y = y_train)

print("Parámetros árbol:",gb.grid.best_params_)
mejor_arbol=gb.grid.best_estimator_

Parámetros RandomForest

gb = Class_Fit(clf = RandomForestClassifier)
param_grid = {
             'criterion':['entropy','gini'],
             'max_depth':[3,4,5,10,20],
             'min_samples_leaf':[5,11,3,1],
             'min_impurity_decrease':[0,0.025,0.0025],
             'min_samples_split':[2,4,10],
             'n_estimators':[100],
              'max_features':['auto', 'sqrt', 'log2'],
              'class_weight':['balanced', 'balanced_subsample']
             }
gb.grid_search(parameters = param_grid, Kfold = 4)
gb.grid_fit(X = X_train, Y = y_train)

#Parámetros Randomforest: {'class_weight': 'balanced_subsample', 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'min_impurity_decrease': 0.025, 'min_samples_leaf': 11, 'min_samples_split': 2, 'n_estimators': 100}


Parámetros XGBOOST

# Se utiliza gridsearch como modelo de selección para determinar los mejores parametros en XGBoost
gb = Class_Fit(clf = XGBClassifier)
param_grid = {
             'objective':['binary:logistic'],
             'n_estimators' : [500],
              'eta':[0.03,0.003],    
              'gamma':[0.03,0.003,5],
              'max_depth':[5,10],
              'min_child_weight':[0.5,5],
              'alpha':[0.003,0.03],
              'scale_pos_weight':[y_train[y_train==0].count()/y_train[y_train==1].count(),0.5],
              'n_job':[-1],
              'booster':['gblinear','gbtree'],  
             }
gb.grid_search(parameters = param_grid, Kfold = 5)
gb.grid_fit(X = X_train, Y = y_train)

--------------
#mejor Modelo árbol
param_arbol={'criterion': 'entropy', 'max_depth': 3,
             'min_impurity_decrease': 0.025, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'best'}

modelo_final_arbol=tree.DecisionTreeClassifier(**param_arbol)
modelo_final_arbol.fit(X_train, y_train)

param_rf={'class_weight': 'balanced_subsample', 'criterion': 'entropy',
'max_depth': 10, 'max_features': 'sqrt', 'min_impurity_decrease': 0.025, 'min_samples_leaf': 11, 'min_samples_split': 10, 'n_estimators': 100}

modelo_final_rf=RandomForestClassifier(**param_rf)
modelo_final_rf.fit(X_train, y_train)

param_xgb={'alpha': 0.003, 'booster': 'gbtree', 'eta': 0.03, 'gamma': 5, 'max_depth': 3,
'min_child_weight': 0.5, 'n_estimators': 500, 'n_job': -1, 'objective': 'binary:logistic', 'scale_pos_weight': 23.74074074074074}

modelo_final_xgb=XGBClassifier(**param_xgb)
modelo_final_xgb.fit(X_train, y_train)


------------------

print("Modelo A")
eval_model(X_test,X_train,y_test,y_train,modelo_final_arbol)
kflods = 4
ab_vec=validacion_cruzada(X_train,y_train, kflods, modelo_final_arbol)
display(ab_vec)
print('Promedio modelo A:', sum(ab_vec)/len(ab_vec)) 
print("_________________________________________")
print("Modelo RF")
eval_model(X_test,X_train,y_test,y_train,modelo_final_rf)
kflods = 4
ab_vec=validacion_cruzada(X_train,y_train, kflods, modelo_final_rf)
display(ab_vec)
print('Promedio modelo RF:', sum(ab_vec)/len(ab_vec)) 
print("_________________________________________")
print("Modelo XGB")
eval_model(X_test,X_train,y_test,y_train,modelo_final_xgb)
kflods = 4
ab_vec=validacion_cruzada(X_train,y_train, kflods, modelo_final_xgb)
display(ab_vec)
print('Promedio modelo RF:', sum(ab_vec)/len(ab_vec)) 
